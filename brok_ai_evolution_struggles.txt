
# ğŸ› ï¸ The Evolution & Struggles Behind Brok AI

Building **Brok AI** wasnâ€™t just a technical project â€” it was a creative hustle, a journey through limitations, learning curves, and constant iterations. From small-scale models to powerful instruction-tuned LLMs, every phase came with its own battle. Hereâ€™s how Brok evolved and what went behind the scenes:

---

## ğŸ”¹ Phase 1: GPT-2 Small â€“ The Humble Beginning

- **Initial choice**: GPT-2 small (117M) for testing basic conversational flows.
- **Struggles**:
  - **Limited context memory** made it hard to sustain meaningful marketing-oriented conversations.
  - Frequent **technical issues** like unstable response formats and abrupt outputs.
  - Responses lacked creativity and sounded robotic â€” not suited for a bot with personality.

---

## ğŸ”¹ Phase 2: GPT-2 Large â€“ Slightly Better, But Still Lacking

- **Upgraded to**: GPT-2 Large for more tokens and better fluency.
- **Challenges encountered**:
  - Faced **tokenization issues** and **temperature/k-sampling inconsistencies**, which caused fluctuating response quality.
  - Still couldnâ€™t generate **strategic-level marketing content** â€” lacked persuasive tone.
  - Struggled with **content moderation** and keeping outputs brand-safe for real-time usage.

---

## ğŸ”¹ Phase 3: GPT-J 6B â€“ A Bigger Brain, But Still Flawed

- **Model shift**: GPT-J 6B to try unlocking deeper coherence and reasoning.
- **What went wrong**:
  - While the responses were **logically richer**, the **content often became bloated or misaligned**.
  - Encountered **over-generation** problems â€” responses would go off-topic or become too verbose.
  - **Latency and memory issues** began popping up on Hugging Face Spaces.
  - Overall, **not production-ready** for a consistent marketing experience.

---

## ğŸ”¹ Phase 4: Mistral-7B Instruct â€“ The Real Game Changer

- **Final shift**: `mistralai/Mistral-7B-Instruct-v0.1`
- **Initial blocker**:
  - Hugging Faceâ€™s **gated access** error (`401 Client Error`) blocked early implementation.
  - Required proper **authentication using HF tokens** and environment secret management.
- **Breakthroughs**:
  - With the help of **4-bit quantization (BitsAndBytesConfig)**, we optimized memory usage and deployed efficiently.
  - This model finally delivered:
    - Strategic marketing tone.
    - Persona-switching capability.
    - Faster response time and better retention of instruction context.
  - Implemented **content filtering**, **response formatting**, and **persona tone adaptation**.

---

### ğŸ’¬ Personal Reflection â€“ by Shakthi

*Brok AI started as a side experiment using GPT-2, but I quickly realized the limitations and outgrew each model phase. From tokenization errors to unpredictable temperature sampling to the frustration of vague, misaligned responses â€” it was a constant cycle of test, fail, tweak, and retry. Only with Mistral-7B did Brok finally become what I envisioned: a high-functioning, tone-flexible, marketing-focused conversational AI.*

---

